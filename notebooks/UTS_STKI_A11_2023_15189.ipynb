{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b5db690",
   "metadata": {},
   "source": [
    "# UTS STKI — A11.2023.15189\n",
    "*Soal 02–05*\n",
    "\n",
    "Notebook ini menggabungkan seluruh implementasi untuk tugas UTS STKI: Preprocessing, Boolean IR, VSM, Search, dan Evaluasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737e8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook working directory: c:\\Users\\ASUS\\OneDrive\\Documents\\KULIAH RENDY\\SEMESTER 5\\SISTEM TEMU KEMBALI INFORMASI\\stki-uts-a11.2023.15189-aditya rendy setyawan\\notebooks\n",
      "Expected data dir: c:\\Users\\ASUS\\OneDrive\\Documents\\KULIAH RENDY\\SEMESTER 5\\SISTEM TEMU KEMBALI INFORMASI\\stki-uts-a11.2023.15189-aditya rendy setyawan\\notebooks\\data\n",
      "Expected data_processed dir: c:\\Users\\ASUS\\OneDrive\\Documents\\KULIAH RENDY\\SEMESTER 5\\SISTEM TEMU KEMBALI INFORMASI\\stki-uts-a11.2023.15189-aditya rendy setyawan\\notebooks\\data_processed\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "DATA_PROCESSED_DIR = os.path.join(PROJECT_ROOT, \"data_processed\")\n",
    "\n",
    "print(\"Notebook working directory:\", PROJECT_ROOT)\n",
    "print(\"Expected data dir:\", DATA_DIR)\n",
    "print(\"Expected data_processed dir:\", DATA_PROCESSED_DIR)\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(DATA_PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    import Sastrawi\n",
    "except Exception:\n",
    "    print(\"\\nPeringatan: paket Sastrawi tidak terpasang di lingkungan ini.\")\n",
    "    print(\"Install dengan: pip install Sastrawi\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f6ca08",
   "metadata": {},
   "source": [
    "## Soal 02 — Preprocessing\n",
    "Langkah-langkah: tokenisasi, normalisasi, hapus stopwords, stemming (Sastrawi), dan menyimpan hasil ke `data_processed/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61ee095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopwords.txt not found at c:\\Users\\ASUS\\OneDrive\\Documents\\KULIAH RENDY\\SEMESTER 5\\SISTEM TEMU KEMBALI INFORMASI\\stki-uts-a11.2023.15189-aditya rendy setyawan\\notebooks\\data\\stopwords.txt. Using default small list.\n",
      "Found raw txt files: []\n"
     ]
    }
   ],
   "source": [
    "import re, glob\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "except Exception as e:\n",
    "    raise ImportError(\"Sastrawi required. Install with: pip install Sastrawi\") from e\n",
    "\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "DATA_DIR = os.path.join(PROJECT_ROOT, \"data\")\n",
    "DATA_PROCESSED_DIR = os.path.join(PROJECT_ROOT, \"data_processed\")\n",
    "os.makedirs(DATA_PROCESSED_DIR, exist_ok=True)\n",
    "\n",
    "def load_stopwords(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"stopwords.txt not found at {path}. Using default small list.\")\n",
    "        return set([\"dan\",\"di\",\"ke\",\"yang\",\"untuk\",\"dari\",\"pada\",\"ini\",\"itu\",\"dengan\"])\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return set(line.strip().lower() for line in f if line.strip())\n",
    "\n",
    "stopwords_path = os.path.join(DATA_DIR, \"stopwords.txt\")\n",
    "stop_words = load_stopwords(stopwords_path)\n",
    "\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if len(t) > 2]\n",
    "    return tokens\n",
    "\n",
    "def remove_stop(tokens, stop_words):\n",
    "    return [t for t in tokens if t not in stop_words]\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "txt_files = sorted(glob.glob(os.path.join(DATA_DIR, \"*.txt\")))\n",
    "print(\"Found raw txt files:\", txt_files)\n",
    "processed = {}\n",
    "for path in txt_files:\n",
    "    name = os.path.basename(path)\n",
    "    if name.lower() == \"stopwords.txt\":\n",
    "        continue\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = f.read()\n",
    "    cleaned = clean_text(raw)\n",
    "    toks = tokenize(cleaned)\n",
    "    toks = remove_stop(toks, stop_words)\n",
    "    toks = stem_tokens(toks)\n",
    "    processed[name] = toks\n",
    "    out_name = f\"CLEAN_{name}\"\n",
    "    out_path = os.path.join(DATA_PROCESSED_DIR, out_name)\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as outf:\n",
    "        outf.write(\" \".join(toks))\n",
    "    print(f\"Saved {out_name} ({len(toks)} tokens) to data_processed/\")\n",
    "\n",
    "from collections import Counter\n",
    "for name, toks in processed.items():\n",
    "    print(\"\\nDocument:\", name)\n",
    "    for term, cnt in Counter(toks).most_common(10):\n",
    "        print(f\"  {term:<15} {cnt}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4925b595",
   "metadata": {},
   "source": [
    "## Soal 03 — Boolean Retrieval\n",
    "Membangun inverted index, incidence matrix, operasi AND/OR/NOT, dan evaluasi precision/recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a48cda4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents loaded: ['RPS Kriptografi', 'RPS Manajemen Proyek Teknologi Infromasi', 'RPS Sistem Informasi', 'RPS Sistem Temu Kembali Informasi', 'RPS Sistem Terdistribusi', 'stopwords']\n",
      "Vocabulary size: 1250\n",
      "\n",
      "Sample inverted index entries:\n",
      "sesuai -> ['D1', 'D2', 'D3', 'D4', 'D5']\n",
      "tunjang -> ['D1']\n",
      "manfaat -> ['D1', 'D2', 'D5']\n",
      "mutu -> ['D1', 'D2', 'D3']\n",
      "dekripsi -> ['D1']\n",
      "deskripsi -> ['D1', 'D2', 'D3', 'D4', 'D5']\n",
      "dosen -> ['D1', 'D2', 'D3', 'D4', 'D5']\n",
      "komunikasi -> ['D1', 'D2', 'D3', 'D5']\n",
      "substitusi -> ['D1']\n",
      "sejarah -> ['D1', 'D5']\n",
      "\n",
      "Query: informasi AND proyek\n",
      " Retrieved: ['D2', 'D3']\n",
      " Precision: 0.0000, Recall: 0.0000\n",
      "\n",
      "Query: kriptografi OR dekripsi\n",
      " Retrieved: ['D1']\n",
      " Precision: 0.0000, Recall: 0.0000\n",
      "\n",
      "Query: NOT proyek\n",
      " Retrieved: ['D1', 'D4', 'D6']\n",
      " Precision: 1.0000, Recall: 0.6000\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "DATA_PROCESSED_DIR = os.path.join(os.getcwd(), \"../data_processed\")\n",
    "\n",
    "def load_processed(directory):\n",
    "    files = sorted([f for f in os.listdir(directory) if f.startswith(\"CLEAN_\") and f.endswith(\".txt\")])\n",
    "    docs = {}\n",
    "    id_map = {}\n",
    "    for i, fn in enumerate(files):\n",
    "        with open(os.path.join(directory, fn), \"r\", encoding=\"utf-8\") as f:\n",
    "            toks = f.read().split()\n",
    "        doc_id = f\"D{i+1}\"\n",
    "        docs[doc_id] = toks\n",
    "        id_map[doc_id] = fn.replace(\"CLEAN_\", \"\").replace(\".txt\",\"\")\n",
    "    vocab = sorted({t for toks in docs.values() for t in toks})\n",
    "    return docs, id_map, vocab\n",
    "\n",
    "docs, id_map, vocab = load_processed(DATA_PROCESSED_DIR)\n",
    "print(\"Documents loaded:\", list(id_map.values()))\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "def build_inverted(docs):\n",
    "    inv = defaultdict(list)\n",
    "    for doc_id, toks in docs.items():\n",
    "        for term in set(toks):\n",
    "            inv[term].append(doc_id)\n",
    "    for term in inv:\n",
    "        inv[term].sort()\n",
    "    return dict(inv)\n",
    "\n",
    "def build_incidence(docs, vocab):\n",
    "    doc_ids = sorted(docs.keys())\n",
    "    mat = np.zeros((len(vocab), len(doc_ids)), dtype=int)\n",
    "    term_to_idx = {t:i for i,t in enumerate(vocab)}\n",
    "    doc_to_idx = {d:i for i,d in enumerate(doc_ids)}\n",
    "    for d, toks in docs.items():\n",
    "        for t in set(toks):\n",
    "            mat[term_to_idx[t], doc_to_idx[d]] = 1\n",
    "    return mat, doc_ids\n",
    "\n",
    "inverted_index = build_inverted(docs)\n",
    "incidence_matrix, doc_ids = build_incidence(docs, vocab)\n",
    "\n",
    "print(\"\\nSample inverted index entries:\")\n",
    "for k in list(inverted_index.keys())[:10]:\n",
    "    print(k, \"->\", inverted_index[k])\n",
    "\n",
    "def boolean_retrieve(query, inverted_index, all_doc_ids):\n",
    "    q = query.lower().split()\n",
    "    if not q:\n",
    "        return []\n",
    "    if len(q) == 1:\n",
    "        return inverted_index.get(q[0], [])\n",
    "    if len(q) == 2 and q[0].upper()==\"NOT\":\n",
    "        return sorted(set(all_doc_ids) - set(inverted_index.get(q[1], [])))\n",
    "    if len(q) == 3:\n",
    "        a, op, b = q[0], q[1].upper(), q[2]\n",
    "        pa = set(inverted_index.get(a, []))\n",
    "        pb = set(inverted_index.get(b, []))\n",
    "        if op==\"AND\":\n",
    "            return sorted(pa & pb)\n",
    "        if op==\"OR\":\n",
    "            return sorted(pa | pb)\n",
    "    return []\n",
    "\n",
    "all_docs = sorted(docs.keys())\n",
    "queries = [(\"informasi AND proyek\", ['D5']), (\"kriptografi OR dekripsi\", ['D4']), (\"NOT proyek\", [d for d in all_docs if d!='D5'])]\n",
    "from collections import Counter\n",
    "\n",
    "for q, gold in queries:\n",
    "    res = boolean_retrieve(q, inverted_index, all_docs)\n",
    "    tp = len(set(res)&set(gold))\n",
    "    precision = tp / (len(res) or 1)\n",
    "    recall = tp / (len(gold) or 1)\n",
    "    print(f\"\\nQuery: {q}\")\n",
    "    print(\" Retrieved:\", res)\n",
    "    print(f\" Precision: {precision:.4f}, Recall: {recall:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385308fc",
   "metadata": {},
   "source": [
    "## Soal 04 — Vector Space Model (TF-IDF) & Cosine Similarity\n",
    "Hitung TF, DF, IDF, buat matriks TF-IDF dan ranking menggunakan cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b1bf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs: ['RPS Kriptografi', 'RPS Manajemen Proyek Teknologi Infromasi', 'RPS Sistem Informasi', 'RPS Sistem Temu Kembali Informasi', 'RPS Sistem Terdistribusi', 'stopwords']\n",
      "Vocab size: 1250\n",
      "TF-IDF shape: (1250, 6)\n",
      "\n",
      "Top results for query: kriptografi\n",
      "('D1', np.float64(0.16997105846329377))\n",
      "('D2', np.float64(0.0))\n",
      "('D3', np.float64(0.0))\n",
      "('D4', np.float64(0.0))\n",
      "('D5', np.float64(0.0))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "DATA_PROCESSED_DIR = os.path.join(os.getcwd(), \"../data_processed\")\n",
    "\n",
    "def load_processed_for_vsm(directory):\n",
    "    files = sorted([f for f in os.listdir(directory) if f.startswith(\"CLEAN_\") and f.endswith(\".txt\")])\n",
    "    docs = {}\n",
    "    id_map = {}\n",
    "    raw_snip = {}\n",
    "    for i, fn in enumerate(files):\n",
    "        with open(os.path.join(directory, fn), \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "        toks = content.split()\n",
    "        doc_id = f\"D{i+1}\"\n",
    "        docs[doc_id] = toks\n",
    "        id_map[doc_id] = fn.replace(\"CLEAN_\",\"\").replace(\".txt\",\"\")\n",
    "        raw_snip[doc_id] = content[:120].replace(\"\\n\",\" \") + \"...\"\n",
    "    vocab = sorted({t for toks in docs.values() for t in toks})\n",
    "    return docs, id_map, vocab, raw_snip\n",
    "\n",
    "docs, id_map, vocab, raw_snip = load_processed_for_vsm(DATA_PROCESSED_DIR)\n",
    "print(\"Docs:\", list(id_map.values()))\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "\n",
    "def calculate_tf_idf(docs, vocab):\n",
    "    N = len(docs)\n",
    "    doc_ids = sorted(docs.keys())\n",
    "    term_to_idx = {t:i for i,t in enumerate(vocab)}\n",
    "    data, rows, cols = [], [], []\n",
    "    df = Counter()\n",
    "    for j, d in enumerate(doc_ids):\n",
    "        tc = Counter(docs[d])\n",
    "        for term in set(docs[d]):\n",
    "            df[term] += 1\n",
    "        for term, cnt in tc.items():\n",
    "            if term in term_to_idx:\n",
    "                tf = 1 + np.log10(cnt) if cnt>0 else 0\n",
    "                rows.append(term_to_idx[term])\n",
    "                cols.append(j)\n",
    "                data.append(tf)\n",
    "    tf_matrix = csr_matrix((data, (rows, cols)), shape=(len(vocab), len(doc_ids)))\n",
    "    idf = np.zeros(len(vocab))\n",
    "    for term, idx in term_to_idx.items():\n",
    "        idf[idx] = np.log10(N / (df[term] or 1))\n",
    "    tfidf = tf_matrix.multiply(idf[:, np.newaxis])\n",
    "    return tfidf, idf, term_to_idx, doc_ids\n",
    "\n",
    "tfidf_matrix, idf_vector, term_to_idx, doc_ids = calculate_tf_idf(docs, vocab)\n",
    "print(\"TF-IDF shape:\", tfidf_matrix.shape)\n",
    "\n",
    "def query_to_tfidf(query, term_to_idx, idf_vector):\n",
    "    q_toks = query.lower().split()\n",
    "    qc = Counter(q_toks)\n",
    "    qvec = np.zeros(len(term_to_idx))\n",
    "    for t, cnt in qc.items():\n",
    "        if t in term_to_idx:\n",
    "            qvec[term_to_idx[t]] = (1 + np.log10(cnt)) * idf_vector[term_to_idx[t]]\n",
    "    from scipy.sparse import csr_matrix\n",
    "    return csr_matrix(qvec).transpose()\n",
    "\n",
    "def rank_documents(query_vec, tfidf_mat, doc_ids):\n",
    "    sims = cosine_similarity(query_vec.transpose(), tfidf_mat.transpose())[0]\n",
    "    ranking = sorted(zip(doc_ids, sims), key=lambda x: x[1], reverse=True)\n",
    "    return ranking\n",
    "\n",
    "q = \"kriptografi\"\n",
    "qvec = query_to_tfidf(q, term_to_idx, idf_vector)\n",
    "rank = rank_documents(qvec, tfidf_matrix, doc_ids)\n",
    "print(\"\\nTop results for query:\", q)\n",
    "for r in rank[:5]:\n",
    "    print(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dffb54",
   "metadata": {},
   "source": [
    "## Soal 05 — Evaluasi Sistem IR\n",
    "Precision, Recall, F1, nDCG. Fungsi evaluasi dapat dipanggil dengan hasil ranking dari VSM atau Boolean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ac0eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@5: 0.2000, Recall@5: 1.0000, F1: 0.3333, nDCG@5: 0.4307\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def precision(retrieved, relevant):\n",
    "    if not retrieved:\n",
    "        return 0.0\n",
    "    return len(set(retrieved) & set(relevant)) / len(retrieved)\n",
    "\n",
    "def recall(retrieved, relevant):\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    return len(set(retrieved) & set(relevant)) / len(relevant)\n",
    "\n",
    "def f1(P, R):\n",
    "    return 0.0 if (P+R)==0 else 2*P*R/(P+R)\n",
    "\n",
    "def dcg_at_k(scores, k):\n",
    "    scores = np.asarray(scores)[:k]\n",
    "    if scores.size==0:\n",
    "        return 0.0\n",
    "    return np.sum(scores / np.log2(np.arange(2, scores.size+2)))\n",
    "\n",
    "def ndcg_at_k(rel_scores, ideal_scores, k):\n",
    "    dcg = dcg_at_k(rel_scores, k)\n",
    "    idcg = dcg_at_k(sorted(ideal_scores, reverse=True), k)\n",
    "    return 0.0 if idcg==0 else dcg/idcg\n",
    "\n",
    "def evaluate(retrieved, gold, k=5):\n",
    "    P = precision(retrieved[:k], gold)\n",
    "    R = recall(retrieved[:k], gold)\n",
    "    F1 = f1(P, R)\n",
    "    rel = [1 if d in gold else 0 for d in retrieved[:k]]\n",
    "    ideal = [1]*min(k, len(gold)) + [0]*(max(0, k-len(gold)))\n",
    "    nDCG = ndcg_at_k(rel, ideal, k)\n",
    "    print(f\"Precision@{k}: {P:.4f}, Recall@{k}: {R:.4f}, F1: {F1:.4f}, nDCG@{k}: {nDCG:.4f}\")\n",
    "    return P, R, F1, nDCG\n",
    "\n",
    "try:\n",
    "    top_docs = [d for d,s in rank[:5]]\n",
    "    gold = ['D4']\n",
    "    evaluate(top_docs, gold, k=5)\n",
    "except Exception as e:\n",
    "    print(\"No ranking found in session. Run VSM cell first.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821336f",
   "metadata": {},
   "source": [
    "### Selesai"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
